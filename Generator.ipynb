{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import *\n",
    "from utils.dataset import load_all\n",
    "from utils.generate import write_file, generate\n",
    "from utils.util import play_music\n",
    "\n",
    "from IPython import display\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from torch import optim\n",
    "%matplotlib inline\n",
    "\n",
    "import torch, torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['data/Bach1']\n",
      "Loading data\n",
      "X_shape:  (340, 128, 48, 3) y_shape:  (340, 128, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "# path to the folder with raw midi file to use \n",
    "styles = [['data/Bach1']]\n",
    "print(styles[0])\n",
    "print('Loading data')\n",
    "X, y = load_all(styles, SEQ_LEN)\n",
    "print('X_shape: ', X.shape, 'y_shape: ', y.shape)\n",
    "\n",
    "k = 1\n",
    "X_tr = X[:-k]\n",
    "X_te = X[-k:]\n",
    "y_tr = y[:-k]\n",
    "y_te = y[-k:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Каждый мимди файл кодируется тремя матрицами (временная ось и нотная ось)\n",
    "\n",
    "- Play matrix - играется ли нота в данный момент или нет\n",
    "- Replay matrix  - Играется ли нота заново или продолжает звучать\n",
    "- Volume matrix\n",
    "\n",
    "Например, в загруженных выше данных 10 - батчсайз,  128 - временная ось, 48 - нотная ось (от низих нот к высоким), 3 - Play, Replay, Volume\n",
    "\n",
    "Подробнее о базовой архитектуре и входе можно прочитать в https://arxiv.org/abs/1801.00887 (С тем отличием, что в приведенном ниже бейзлайне отсутствует компанента, отвечающая за стиль музыки)\n",
    "\n",
    "Базовую архитектуру можно пробовать улучшить следующим образом:\n",
    "\n",
    "- Добавить self_attention модуль\n",
    "- Заменить LSTM на энкодер трансформера (будет быстрее работать)\n",
    "- Подумать как добавить глобальный модуль, который кодирует информацию о всем фрагменте\n",
    "- Добавить стиль музыки (напр. как в https://arxiv.org/abs/1801.00887)\n",
    "- Beam search\n",
    "\n",
    "Еще:\n",
    "- GAN (семплирование для дискриминатора можно попробовать делать по порогу вероятности)\n",
    "\n",
    "Если хотим семплировать трушно: \n",
    "- GAN + RL (arXiv:1701.06547)\n",
    "\n",
    "Если используем RL не забываем про:\n",
    "- Teacher Forcing\n",
    "- Дискриминатор и Генератор нужно претрейнить\n",
    "- Мультилосс - для Генератора использовать стандартный лосс вместе с лоссом для GAN-RL \n",
    "- Используйте бейзлайн для RL лосса, чтобы уменьшить его дисперсию  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, torch.nn as nn\n",
    "from constants import *\n",
    "import numpy as np\n",
    "\n",
    "from model.modules.feature_generation_module import feature_generation, beat_f\n",
    "\n",
    "\n",
    "def sample_sound(data_gen):\n",
    "    \"\"\"sample music from distribution\"\"\"\n",
    "    size = data_gen.size()\n",
    "    assert len(size) in [3,4], 'dimension of input tensor for sampling must be 3 or 4 dimensional'\n",
    "    rand = torch.from_numpy(np.random.random(size)).type(torch.FloatTensor)\n",
    "    sample = (rand < data_gen).type(torch.FloatTensor)\n",
    "    \n",
    "    if len(size) == 4:\n",
    "        vol = sample[:, :, :, :1]\n",
    "    elif len(size) == 3:\n",
    "        vol = 0.6*sample[:, :, :1] #0.6 is the mean volume among played notes\n",
    "    sample = torch.cat([sample, vol], dim=-1)\n",
    "    \n",
    "    if CUDA:        \n",
    "        return sample.cuda()        \n",
    "    else:\n",
    "        return sample\n",
    "\n",
    "\n",
    "class time_axis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self.__class__, self).__init__() \n",
    "        self.n_layers = TIME_AXIS_LAYERS\n",
    "        self.hidden_size = TIME_AXIS_UNITS\n",
    "\n",
    "        self.input_size = NOTE_EMBEDDING\n",
    "        self.input_size += BEATS_FEATURES\n",
    "\n",
    "        self.time_lstm = nn.LSTM(self.input_size, self.hidden_size, self.n_layers, dropout=0.1, \n",
    "                                 batch_first=True, )\n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        self.generate_features = feature_generation()\n",
    "        self.beat_embedding = nn.Embedding(num_embeddings=16, embedding_dim = 16)\n",
    "\n",
    "        \n",
    "    def forward(self, notes, beats = None):\n",
    "        \n",
    "        \"\"\"\n",
    "        arg:\n",
    "            notes - (batch, time_seq, note_seq, note_features)\n",
    "        \n",
    "        out: \n",
    "            (batch, time_seq, note_seq, hidden_features)\n",
    "            \n",
    "        \"\"\"\n",
    "        \n",
    "        notes = self.dropout(notes)\n",
    "\n",
    "        note_features = self.generate_features(notes)\n",
    "        notes = note_features\n",
    "\n",
    "        if beats is None:\n",
    "            beats = beat_f(notes)\n",
    "        else:\n",
    "            # used when music is generating\n",
    "            beats = beats.repeat((notes.shape[2], 1, 1)).permute(1, 2, 0).contiguous()\n",
    "        beats = self.beat_embedding(beats)\n",
    "        \n",
    "        notes = torch.cat([notes, beats], dim=-1)\n",
    "        initial_shape = notes.shape\n",
    "        \n",
    "        notes = notes.permute(0, 2, 1, 3).contiguous()\n",
    "        notes = notes.view((-1,)+notes.shape[-2:]).contiguous()\n",
    "\n",
    "        lstm_out, hidden = self.time_lstm(notes)\n",
    "                \n",
    "        time_output = lstm_out.contiguous().view((initial_shape[0],) + (initial_shape[2],) + lstm_out.shape[-2:])\n",
    "        time_output = time_output.permute(0, 2, 1, 3)        \n",
    "        \n",
    "        return time_output\n",
    "\n",
    "\n",
    "class note_axis(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self.__class__, self).__init__()   \n",
    "        \n",
    "        \n",
    "        self.n_layers = NOTE_AXIS_LAYERS\n",
    "        self.hidden_size = NOTE_AXIS_UNITS\n",
    "        # number of time features plus number of previous higher note in the same time momemt\n",
    "        self.input_size = TIME_AXIS_UNITS + NOTE_UNITS\n",
    "       \n",
    "        self.note_lstm = nn.LSTM(self.input_size, self.hidden_size, self.n_layers, dropout=0.1, \n",
    "                                 batch_first=True, )\n",
    "        \n",
    "        self.dropout = nn.Dropout(p=0.2)\n",
    "        \n",
    "        self.logits = nn.Linear(self.hidden_size, NOTE_UNITS-1)\n",
    "\n",
    "        # mode\n",
    "        self.to_train = True\n",
    "        self.apply_T = False\n",
    "        self.temperature = 1\n",
    "        self.silent_time = 0\n",
    "        \n",
    "    def generate_music(self, notes):\n",
    "        \n",
    "        initial_shape = notes.shape    \n",
    "        note_input = notes.contiguous().view((-1,)+notes.shape[-2:]).contiguous()\n",
    "            \n",
    "        if CUDA:\n",
    "            hidden = (torch.zeros(self.n_layers, note_input.shape[0], self.hidden_size).cuda(),\n",
    "                      torch.zeros(self.n_layers, note_input.shape[0], self.hidden_size).cuda())\n",
    "        else:\n",
    "            hidden = (torch.zeros(self.n_layers, note_input.shape[0], self.hidden_size),\n",
    "                      torch.zeros(self.n_layers, note_input.shape[0], self.hidden_size))\n",
    "\n",
    "        notes_list = []\n",
    "        sound_list = []\n",
    "        if CUDA:\n",
    "            sound = torch.zeros(note_input[:, 0:1, :].shape[:-1]+(NOTE_UNITS,)).cuda()\n",
    "        else:\n",
    "            sound = torch.zeros(note_input[:, 0:1, :].shape[:-1]+(NOTE_UNITS,))\n",
    "\n",
    "        for i in range(NUM_OCTAVES*OCTAVE):\n",
    "\n",
    "            inputs = torch.cat([note_input[:, i:i+1, :], sound], dim=-1)\n",
    "            note_output, hidden = self.note_lstm(inputs, hidden) \n",
    "\n",
    "            logits = self.logits(note_output) \n",
    "            if self.apply_T:\n",
    "                next_notes = nn.Sigmoid()(logits/self.temperature)\n",
    "            else:\n",
    "                next_notes = nn.Sigmoid()(logits)\n",
    "\n",
    "            sound = sample_sound(next_notes)\n",
    "            notes_list.append(next_notes)\n",
    "            sound_list.append(sound)   \n",
    "\n",
    "        out = torch.cat(notes_list, dim=1)\n",
    "        sounds = torch.cat(sound_list, dim=1)\n",
    "\n",
    "        if self.apply_T:\n",
    "            if (sounds[-1, :, 0] != 0).sum() == 0:\n",
    "                self.silent_time += 1\n",
    "                if self.silent_time >= NOTES_PER_BAR:\n",
    "                    self.temperature += 0.1\n",
    "            else:\n",
    "                self.silent_time = 0\n",
    "                self.temperature = 1 \n",
    "\n",
    "                \n",
    "        note_output = out.contiguous().view(initial_shape[:2] + out.shape[-2:])\n",
    "        sounds = sounds.contiguous().view(initial_shape[:2] + sounds.shape[-2:])\n",
    "        \n",
    "        return note_output, sounds\n",
    "        \n",
    "    def forward(self, notes, chosen):\n",
    "        \"\"\"\n",
    "        arg:\n",
    "            notes: (batch, time_seq, note_seq, time_hidden_features)\n",
    "            chosen: (batch, time_seq, note_seq, time_hidden_features)\n",
    "        \n",
    "        returns:\n",
    "            (batch, time_seq, note_seq, next_notes_features)\n",
    "            \n",
    "        \"\"\"\n",
    "    \n",
    "        if self.to_train:\n",
    "            # Shift target one note to the left.\n",
    "            chosen = self.dropout(chosen)\n",
    "            shift_chosen = nn.ZeroPad2d((0, 0, 1, 0))(chosen[:, :, :-1, :]) \n",
    "            notes = torch.cat([notes, shift_chosen], dim=-1)\n",
    "        \n",
    "            initial_shape = notes.shape    \n",
    "            note_input = notes.contiguous().view((-1,)+notes.shape[-2:]).contiguous()\n",
    "            \n",
    "            out, hidden = self.note_lstm(note_input) \n",
    "            note_output = out.contiguous().view(initial_shape[:2] + out.shape[-2:])\n",
    "            logits = self.logits(note_output) \n",
    "            next_notes = nn.Sigmoid()(logits)    \n",
    "            note_output, sounds = next_notes, sample_sound(next_notes)\n",
    "        else:\n",
    "            # used when music is generating\n",
    "            note_output, sounds = self.generate_music(notes)   \n",
    "            \n",
    "        return note_output, sounds\n",
    "\n",
    "\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(self.__class__, self).__init__()        \n",
    "        \n",
    "        self.time_ax = time_axis() \n",
    "        self.note_ax = note_axis()\n",
    "        \n",
    "    def forward(self, notes, chosen=None, beat=None):\n",
    "        \"\"\"\n",
    "        arg:\n",
    "            notes: tensor\n",
    "            chosen: tensor (predictions - moved notes tensor with one new column)\n",
    "            beat: tensor \n",
    "        \"\"\"\n",
    "\n",
    "        note_ax_output = self.time_ax(notes, beat)\n",
    "        output = self.note_ax(note_ax_output, chosen)                                                             \n",
    "        \n",
    "        return output \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from model.generator import Generator\n",
    "generator = Generator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Metrics:\n",
    "1. EB: ratio of empty bars (in %).\n",
    "2. UPC: number of used pitch classes per bar (from 0 to 12).\n",
    "3. QN: ratio of “qualifed” notes (in %). We consider a note no shorter than three time steps as a qualifed note. QN shows if the music is overly fragmented.\n",
    "\n",
    "More information about metrics can be found at https://arxiv.org/abs/1709.06298"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model.train_utils import train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [05:23, 19.01s/it]"
     ]
    }
   ],
   "source": [
    "generator, epoch, epoch_history = train(generator, X_tr, X_te, y_tr, y_te, \n",
    "                                 batchsize=8, n_epochs=2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample the music"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|          | 0/64 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating music...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 64/64 [00:20<00:00,  3.12it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing file out/samples/output/canonical_test.mid\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "generator.eval()\n",
    "path = 'out/samples/output/canonical_test'\n",
    "write_file(path, generate(generator, 4, to_train=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Play"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Music file out/samples/output/canonical_test.mid loaded!\n"
     ]
    }
   ],
   "source": [
    "midi_file = 'out/samples/output/canonical_test.mid'\n",
    "play_music(midi_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
